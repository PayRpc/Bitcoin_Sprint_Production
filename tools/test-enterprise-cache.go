package main

import (
	"context"
	"encoding/json"
	"fmt"
	"log"
	"runtime"
	"strings"
	"sync"
	"time"

	"github.com/PayRpc/Bitcoin-Sprint/internal/blocks"
	"github.com/PayRpc/Bitcoin-Sprint/internal/cache"
	"go.uber.org/zap"
)

// TestConfig holds comprehensive cache testing configuration
type TestConfig struct {
	MaxBlocks        int           `json:"max_blocks"`
	TestDuration     time.Duration `json:"test_duration"`
	ConcurrentUsers  int           `json:"concurrent_users"`
	WriteRatio       float64       `json:"write_ratio"`
	EnableCompression bool         `json:"enable_compression"`
	EnableL2Cache    bool          `json:"enable_l2_cache"`
	EnableL3Cache    bool          `json:"enable_l3_cache"`
	TestTiering      bool          `json:"test_tiering"`
	StressTest       bool          `json:"stress_test"`
}

// TestResults captures comprehensive test metrics
type TestResults struct {
	Duration         time.Duration            `json:"duration"`
	TotalOperations  int64                    `json:"total_operations"`
	ReadOperations   int64                    `json:"read_operations"`
	WriteOperations  int64                    `json:"write_operations"`
	CacheHits        int64                    `json:"cache_hits"`
	CacheMisses      int64                    `json:"cache_misses"`
	HitRate          float64                  `json:"hit_rate"`
	Throughput       float64                  `json:"throughput_ops_per_sec"`
	AvgLatency       time.Duration            `json:"avg_latency"`
	P50Latency       time.Duration            `json:"p50_latency"`
	P95Latency       time.Duration            `json:"p95_latency"`
	P99Latency       time.Duration            `json:"p99_latency"`
	MemoryUsage      int64                    `json:"memory_usage"`
	Compressions     int64                    `json:"compressions"`
	Decompressions   int64                    `json:"decompressions"`
	Evictions        int64                    `json:"evictions"`
	Errors           int64                    `json:"errors"`
	CacheMetrics     *cache.CacheMetrics      `json:"cache_metrics"`
	PerformanceGraph []PerformanceDataPoint   `json:"performance_graph"`
}

// PerformanceDataPoint represents a point in time performance measurement
type PerformanceDataPoint struct {
	Timestamp  time.Time `json:"timestamp"`
	HitRate    float64   `json:"hit_rate"`
	Throughput float64   `json:"throughput"`
	MemoryUsage int64    `json:"memory_usage"`
	Latency    time.Duration `json:"latency"`
}

// CacheTestSuite provides comprehensive enterprise cache testing
type CacheTestSuite struct {
	cache      *cache.EnterpriseCache
	config     *TestConfig
	logger     *zap.Logger
	results    *TestResults
	latencies  []time.Duration
	mu         sync.RWMutex
	ctx        context.Context
	cancel     context.CancelFunc
}

func main() {
	// Initialize logger
	logger, err := zap.NewDevelopment()
	if err != nil {
		log.Fatalf("Failed to create logger: %v", err)
	}
	defer logger.Sync()

	logger.Info("üöÄ Starting Enterprise Cache Test Suite")

	// Test configurations
	configs := []*TestConfig{
		{
			MaxBlocks:        1000,
			TestDuration:     30 * time.Second,
			ConcurrentUsers:  10,
			WriteRatio:       0.3,
			EnableCompression: true,
			EnableL2Cache:    false,
			EnableL3Cache:    false,
			TestTiering:      false,
			StressTest:       false,
		},
		{
			MaxBlocks:        5000,
			TestDuration:     60 * time.Second,
			ConcurrentUsers:  50,
			WriteRatio:       0.2,
			EnableCompression: true,
			EnableL2Cache:    true,
			EnableL3Cache:    false,
			TestTiering:      true,
			StressTest:       true,
		},
		{
			MaxBlocks:        10000,
			TestDuration:     120 * time.Second,
			ConcurrentUsers:  100,
			WriteRatio:       0.1,
			EnableCompression: true,
			EnableL2Cache:    true,
			EnableL3Cache:    true,
			TestTiering:      true,
			StressTest:       true,
		},
	}

	// Run test suite for each configuration
	for i, config := range configs {
		logger.Info("üß™ Running test configuration",
			zap.Int("config_number", i+1),
			zap.Int("max_blocks", config.MaxBlocks),
			zap.Duration("duration", config.TestDuration),
			zap.Int("concurrent_users", config.ConcurrentUsers))

		suite := NewCacheTestSuite(config, logger)
		if err := suite.RunComprehensiveTests(); err != nil {
			logger.Error("Test suite failed", zap.Error(err))
			continue
		}

		suite.PrintResults()
		suite.SaveResultsToFile(fmt.Sprintf("cache_test_results_%d.json", i+1))
	}

	logger.Info("‚úÖ Enterprise Cache Test Suite completed successfully!")
}

// NewCacheTestSuite creates a new cache test suite
func NewCacheTestSuite(config *TestConfig, logger *zap.Logger) *CacheTestSuite {
	ctx, cancel := context.WithCancel(context.Background())
	
	return &CacheTestSuite{
		config:    config,
		logger:    logger,
		results:   &TestResults{},
		latencies: make([]time.Duration, 0),
		ctx:       ctx,
		cancel:    cancel,
	}
}

// RunComprehensiveTests executes the full test suite
func (suite *CacheTestSuite) RunComprehensiveTests() error {
	// Initialize cache with test configuration
	if err := suite.initializeCache(); err != nil {
		return fmt.Errorf("failed to initialize cache: %w", err)
	}
	defer suite.cleanupCache()

	suite.logger.Info("üìä Starting comprehensive cache tests")

	// Phase 1: Basic functionality tests
	suite.logger.Info("üîß Phase 1: Basic functionality validation")
	if err := suite.testBasicFunctionality(); err != nil {
		return fmt.Errorf("basic functionality test failed: %w", err)
	}

	// Phase 2: Performance tests
	suite.logger.Info("‚ö° Phase 2: Performance benchmarking")
	if err := suite.testPerformance(); err != nil {
		return fmt.Errorf("performance test failed: %w", err)
	}

	// Phase 3: Concurrency tests
	suite.logger.Info("üîÑ Phase 3: Concurrency stress testing")
	if err := suite.testConcurrency(); err != nil {
		return fmt.Errorf("concurrency test failed: %w", err)
	}

	// Phase 4: Compression tests
	if suite.config.EnableCompression {
		suite.logger.Info("üóúÔ∏è Phase 4: Compression validation")
		if err := suite.testCompression(); err != nil {
			return fmt.Errorf("compression test failed: %w", err)
		}
	}

	// Phase 5: Tiered caching tests
	if suite.config.TestTiering {
		suite.logger.Info("üèóÔ∏è Phase 5: Tiered caching validation")
		if err := suite.testTieredCaching(); err != nil {
			return fmt.Errorf("tiered caching test failed: %w", err)
		}
	}

	// Phase 6: Circuit breaker tests
	suite.logger.Info("üîå Phase 6: Circuit breaker validation")
	if err := suite.testCircuitBreaker(); err != nil {
		return fmt.Errorf("circuit breaker test failed: %w", err)
	}

	// Phase 7: Health monitoring tests
	suite.logger.Info("üè• Phase 7: Health monitoring validation")
	if err := suite.testHealthMonitoring(); err != nil {
		return fmt.Errorf("health monitoring test failed: %w", err)
	}

	// Phase 8: Stress testing
	if suite.config.StressTest {
		suite.logger.Info("üí™ Phase 8: Extreme stress testing")
		if err := suite.testStressScenarios(); err != nil {
			return fmt.Errorf("stress test failed: %w", err)
		}
	}

	// Finalize results
	suite.finalizeResults()

	return nil
}

// initializeCache sets up the cache with test configuration
func (suite *CacheTestSuite) initializeCache() error {
	cacheConfig := cache.DefaultCacheConfig()
	cacheConfig.MaxEntries = suite.config.MaxBlocks
	cacheConfig.MaxSize = int64(suite.config.MaxBlocks * 1024) // 1KB per block estimate
	
	if suite.config.EnableCompression {
		cacheConfig.CompressionType = 1 // Gzip
		cacheConfig.CompressionThreshold = 512
	}
	
	cacheConfig.EnableL2Disk = suite.config.EnableL2Cache
	cacheConfig.EnableL3Distributed = suite.config.EnableL3Cache
	cacheConfig.EnableCircuitBreaker = true
	cacheConfig.EnableHealthChecks = true
	cacheConfig.EnableMetrics = true

	var err error
	suite.cache, err = cache.NewEnterpriseCache(cacheConfig, suite.logger)
	if err != nil {
		return fmt.Errorf("failed to create enterprise cache: %w", err)
	}

	return nil
}

// testBasicFunctionality validates core cache operations
func (suite *CacheTestSuite) testBasicFunctionality() error {
	// Test 1: Set and Get
	testBlock := createTestBlock(1, "bitcoin")
	err := suite.cache.SetLatestBlock(testBlock)
	if err != nil {
		return fmt.Errorf("failed to set latest block: %w", err)
	}

	retrievedBlock, found := suite.cache.GetLatestBlock()
	if !found {
		return fmt.Errorf("failed to retrieve latest block")
	}

	if retrievedBlock.Height != testBlock.Height {
		return fmt.Errorf("retrieved block height mismatch: expected %d, got %d", 
			testBlock.Height, retrievedBlock.Height)
	}

	// Test 2: Generic cache operations
	if err := suite.cache.Set("test_key", "test_value", time.Minute); err != nil {
		return fmt.Errorf("failed to set cache entry: %w", err)
	}

	value, found := suite.cache.Get("test_key")
	if !found {
		return fmt.Errorf("failed to retrieve cache entry")
	}

	if value != "test_value" {
		return fmt.Errorf("cache value mismatch: expected 'test_value', got %v", value)
	}

	// Test 3: TTL expiration
	if err := suite.cache.Set("ttl_test", "expires_soon", 100*time.Millisecond); err != nil {
		return fmt.Errorf("failed to set TTL test entry: %w", err)
	}

	time.Sleep(200 * time.Millisecond)
	
	_, found = suite.cache.Get("ttl_test")
	if found {
		return fmt.Errorf("TTL entry should have expired but was still found")
	}

	suite.logger.Info("‚úÖ Basic functionality tests passed")
	return nil
}

// testPerformance benchmarks cache performance
func (suite *CacheTestSuite) testPerformance() error {
	const iterations = 10000
	start := time.Now()

	// Populate cache
	for i := 0; i < iterations; i++ {
		block := createTestBlock(uint32(i), "bitcoin")
		key := fmt.Sprintf("block_%d", i)
		
		startOp := time.Now()
		err := suite.cache.Set(key, block, time.Hour)
		latency := time.Since(startOp)
		
		if err != nil {
			suite.results.Errors++
		} else {
			suite.recordLatency(latency)
			suite.results.WriteOperations++
		}
	}

	// Read test
	for i := 0; i < iterations; i++ {
		key := fmt.Sprintf("block_%d", i)
		
		startOp := time.Now()
		_, found := suite.cache.Get(key)
		latency := time.Since(startOp)
		
		suite.recordLatency(latency)
		suite.results.ReadOperations++
		
		if found {
			suite.results.CacheHits++
		} else {
			suite.results.CacheMisses++
		}
	}

	duration := time.Since(start)
	suite.results.Duration = duration
	suite.results.TotalOperations = suite.results.ReadOperations + suite.results.WriteOperations
	suite.results.Throughput = float64(suite.results.TotalOperations) / duration.Seconds()

	if suite.results.TotalOperations > 0 {
		suite.results.HitRate = float64(suite.results.CacheHits) / float64(suite.results.ReadOperations)
	}

	suite.logger.Info("‚úÖ Performance tests completed",
		zap.Int64("operations", suite.results.TotalOperations),
		zap.Float64("throughput", suite.results.Throughput),
		zap.Float64("hit_rate", suite.results.HitRate))

	return nil
}

// testConcurrency validates cache behavior under concurrent load
func (suite *CacheTestSuite) testConcurrency() error {
	const opsPerWorker = 1000
	workers := suite.config.ConcurrentUsers
	
	var wg sync.WaitGroup
	errorsChan := make(chan error, workers)
	
	start := time.Now()

	// Launch concurrent workers
	for i := 0; i < workers; i++ {
		wg.Add(1)
		go func(workerID int) {
			defer wg.Done()
			
			for j := 0; j < opsPerWorker; j++ {
				// Mix of reads and writes based on write ratio
				if float64(j%100)/100.0 < suite.config.WriteRatio {
					// Write operation
					key := fmt.Sprintf("worker_%d_key_%d", workerID, j)
					block := createTestBlock(uint32(j), "concurrent_test")
					
					startOp := time.Now()
					err := suite.cache.Set(key, block, time.Hour)
					latency := time.Since(startOp)
					
					suite.recordLatency(latency)
					
					if err != nil {
						errorsChan <- err
						return
					}
				} else {
					// Read operation
					key := fmt.Sprintf("worker_%d_key_%d", workerID, j/2)
					
					startOp := time.Now()
					_, found := suite.cache.Get(key)
					latency := time.Since(startOp)
					
					suite.recordLatency(latency)
					
					if found {
						suite.results.CacheHits++
					} else {
						suite.results.CacheMisses++
					}
				}
			}
		}(i)
	}

	// Wait for completion
	wg.Wait()
	close(errorsChan)

	// Check for errors
	for err := range errorsChan {
		suite.logger.Error("Concurrency test error", zap.Error(err))
		suite.results.Errors++
	}

	duration := time.Since(start)
	totalOps := int64(workers * opsPerWorker)
	
	suite.logger.Info("‚úÖ Concurrency tests completed",
		zap.Int("workers", workers),
		zap.Int64("total_operations", totalOps),
		zap.Duration("duration", duration),
		zap.Int64("errors", suite.results.Errors))

	return nil
}

// testCompression validates compression functionality
func (suite *CacheTestSuite) testCompression() error {
	// Create large block that should trigger compression
	largeBlock := createLargeTestBlock(1, "bitcoin", 2048) // 2KB block
	
	err := suite.cache.SetLatestBlock(largeBlock)
	if err != nil {
		return fmt.Errorf("failed to set large block: %w", err)
	}

	// Retrieve and verify
	retrievedBlock, found := suite.cache.GetLatestBlock()
	if !found {
		return fmt.Errorf("failed to retrieve compressed block")
	}

	if retrievedBlock.Height != largeBlock.Height {
		return fmt.Errorf("compressed block data corrupted")
	}

	metrics := suite.cache.GetMetrics()
	suite.results.Compressions = metrics.Compressions
	suite.results.Decompressions = metrics.Decompressions

	suite.logger.Info("‚úÖ Compression tests completed",
		zap.Int64("compressions", metrics.Compressions),
		zap.Int64("decompressions", metrics.Decompressions))

	return nil
}

// testTieredCaching validates multi-level caching
func (suite *CacheTestSuite) testTieredCaching() error {
	// This would test L1 -> L2 -> L3 promotion and demotion
	// For now, just validate that tiered operations don't fail
	
	for i := 0; i < 100; i++ {
		key := fmt.Sprintf("tiered_test_%d", i)
		block := createTestBlock(uint32(i), "tiered")
		
		if err := suite.cache.Set(key, block, time.Hour); err != nil {
			return fmt.Errorf("tiered cache set failed: %w", err)
		}
		
		if _, found := suite.cache.Get(key); !found {
			return fmt.Errorf("tiered cache get failed for key %s", key)
		}
	}

	suite.logger.Info("‚úÖ Tiered caching tests completed")
	return nil
}

// testCircuitBreaker validates circuit breaker functionality
func (suite *CacheTestSuite) testCircuitBreaker() error {
	// Circuit breaker testing would involve simulating failures
	// For now, just verify normal operations work
	
	for i := 0; i < 50; i++ {
		key := fmt.Sprintf("circuit_test_%d", i)
		if err := suite.cache.Set(key, "test_value", time.Hour); err != nil {
			return fmt.Errorf("circuit breaker test failed: %w", err)
		}
	}

	suite.logger.Info("‚úÖ Circuit breaker tests completed")
	return nil
}

// testHealthMonitoring validates health checking functionality
func (suite *CacheTestSuite) testHealthMonitoring() error {
	// Get initial health metrics
	metrics := suite.cache.GetMetrics()
	
	if metrics == nil {
		return fmt.Errorf("failed to retrieve cache metrics")
	}

	// Verify health score is reasonable
	if metrics.HealthScore < 0.0 || metrics.HealthScore > 1.0 {
		return fmt.Errorf("invalid health score: %f", metrics.HealthScore)
	}

	suite.logger.Info("‚úÖ Health monitoring tests completed",
		zap.Float64("health_score", metrics.HealthScore),
		zap.Float64("hit_rate", metrics.HitRate),
		zap.Int64("memory_usage", metrics.MemoryUsage))

	return nil
}

// testStressScenarios runs extreme stress tests
func (suite *CacheTestSuite) testStressScenarios() error {
	suite.logger.Info("üî• Running extreme stress scenarios")

	// Scenario 1: Memory pressure test
	for i := 0; i < suite.config.MaxBlocks*2; i++ {
		key := fmt.Sprintf("stress_%d", i)
		block := createLargeTestBlock(uint32(i), "stress", 1024)
		
		if err := suite.cache.Set(key, block, time.Hour); err != nil {
			suite.results.Errors++
		}
	}

	// Scenario 2: Rapid expiration test
	for i := 0; i < 1000; i++ {
		key := fmt.Sprintf("expire_%d", i)
		if err := suite.cache.Set(key, "expires", time.Millisecond); err != nil {
			suite.results.Errors++
		}
	}

	time.Sleep(100 * time.Millisecond) // Let items expire

	// Scenario 3: Mixed workload burst
	for i := 0; i < 5000; i++ {
		if i%3 == 0 {
			suite.cache.Set(fmt.Sprintf("burst_%d", i), "value", time.Hour)
		} else {
			suite.cache.Get(fmt.Sprintf("burst_%d", i/3))
		}
	}

	suite.logger.Info("‚úÖ Stress scenarios completed")
	return nil
}

// Helper functions

func (suite *CacheTestSuite) recordLatency(latency time.Duration) {
	suite.mu.Lock()
	defer suite.mu.Unlock()
	suite.latencies = append(suite.latencies, latency)
}

func (suite *CacheTestSuite) finalizeResults() {
	suite.mu.RLock()
	defer suite.mu.RUnlock()

	if len(suite.latencies) > 0 {
		// Calculate latency percentiles
		latencies := make([]time.Duration, len(suite.latencies))
		copy(latencies, suite.latencies)
		
		// Simple sort for percentiles
		for i := 0; i < len(latencies)-1; i++ {
			for j := i + 1; j < len(latencies); j++ {
				if latencies[j] < latencies[i] {
					latencies[i], latencies[j] = latencies[j], latencies[i]
				}
			}
		}

		total := time.Duration(0)
		for _, latency := range latencies {
			total += latency
		}
		
		suite.results.AvgLatency = total / time.Duration(len(latencies))
		suite.results.P50Latency = latencies[len(latencies)/2]
		suite.results.P95Latency = latencies[int(float64(len(latencies))*0.95)]
		suite.results.P99Latency = latencies[int(float64(len(latencies))*0.99)]
	}

	// Get final cache metrics
	suite.results.CacheMetrics = suite.cache.GetMetrics()
	suite.results.Evictions = suite.results.CacheMetrics.Evictions

	// Memory usage
	var memStats runtime.MemStats
	runtime.ReadMemStats(&memStats)
	suite.results.MemoryUsage = int64(memStats.Alloc)
}

func (suite *CacheTestSuite) cleanupCache() {
	if suite.cache != nil {
		ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
		defer cancel()
		
		if err := suite.cache.Shutdown(ctx); err != nil {
			suite.logger.Error("Failed to shutdown cache cleanly", zap.Error(err))
		}
	}
	suite.cancel()
}

func (suite *CacheTestSuite) PrintResults() {
	divider := strings.Repeat("=", 80)
	fmt.Printf("\n" + divider + "\n")
	fmt.Printf("üèÜ ENTERPRISE CACHE TEST RESULTS\n")
	fmt.Printf(divider + "\n")
	fmt.Printf("üìä Performance Metrics:\n")
	fmt.Printf("   Total Operations: %d\n", suite.results.TotalOperations)
	fmt.Printf("   Read Operations:  %d\n", suite.results.ReadOperations)
	fmt.Printf("   Write Operations: %d\n", suite.results.WriteOperations)
	fmt.Printf("   Cache Hits:       %d\n", suite.results.CacheHits)
	fmt.Printf("   Cache Misses:     %d\n", suite.results.CacheMisses)
	fmt.Printf("   Hit Rate:         %.2f%%\n", suite.results.HitRate*100)
	fmt.Printf("   Throughput:       %.2f ops/sec\n", suite.results.Throughput)
	fmt.Printf("\n‚è±Ô∏è  Latency Analysis:\n")
	fmt.Printf("   Average:  %v\n", suite.results.AvgLatency)
	fmt.Printf("   P50:      %v\n", suite.results.P50Latency)
	fmt.Printf("   P95:      %v\n", suite.results.P95Latency)
	fmt.Printf("   P99:      %v\n", suite.results.P99Latency)
	fmt.Printf("\nüíæ Resource Usage:\n")
	fmt.Printf("   Memory Usage:     %d bytes\n", suite.results.MemoryUsage)
	fmt.Printf("   Compressions:     %d\n", suite.results.Compressions)
	fmt.Printf("   Decompressions:   %d\n", suite.results.Decompressions)
	fmt.Printf("   Evictions:        %d\n", suite.results.Evictions)
	fmt.Printf("   Errors:           %d\n", suite.results.Errors)
	
	if suite.results.CacheMetrics != nil {
		fmt.Printf("\nüè• Health Metrics:\n")
		fmt.Printf("   Health Score:     %.2f\n", suite.results.CacheMetrics.HealthScore)
		fmt.Printf("   Error Rate:       %.2f%%\n", suite.results.CacheMetrics.ErrorRate*100)
		fmt.Printf("   L1 Hits:          %d\n", suite.results.CacheMetrics.L1Hits)
		fmt.Printf("   L2 Hits:          %d\n", suite.results.CacheMetrics.L2Hits)
		fmt.Printf("   L3 Hits:          %d\n", suite.results.CacheMetrics.L3Hits)
	}
	
	fmt.Printf(strings.Repeat("=", 80) + "\n\n")
}

func (suite *CacheTestSuite) SaveResultsToFile(filename string) error {
	data, err := json.MarshalIndent(suite.results, "", "  ")
	if err != nil {
		return fmt.Errorf("failed to marshal results: %w", err)
	}

	// Note: In a real implementation, you'd write to file
	// For this test, we'll just log that we would save
	suite.logger.Info("üíæ Test results ready for saving",
		zap.String("filename", filename),
		zap.Int("data_size", len(data)))

	return nil
}

// createTestBlock creates a test blockchain event
func createTestBlock(height uint32, chain string) blocks.BlockEvent {
	return blocks.BlockEvent{
		Height:      height,
		Hash:        fmt.Sprintf("test_hash_%d", height),
		Chain:       blocks.Chain(chain),
		Timestamp:   time.Now(),
		DetectedAt:  time.Now(),
		Source:      "test",
		Tier:        "ENTERPRISE",
		RelayTimeMs: 50,
		Status:      blocks.StatusProcessed,
	}
}

// createLargeTestBlock creates a large test block for compression testing
func createLargeTestBlock(height uint32, chain string, size int) blocks.BlockEvent {
	block := createTestBlock(height, chain)
	
	// Add large data to trigger compression
	largeData := make([]byte, size)
	for i := range largeData {
		largeData[i] = byte(i % 256)
	}
	
	block.Hash = fmt.Sprintf("large_test_hash_%d_%x", height, largeData[:8])
	return block
}
